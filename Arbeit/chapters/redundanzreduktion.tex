\chapter{Redundanzreduktion}
\label{kap:Redundanzreduktion}
Die Redundanzreduktion ist ein weiterer wesentlicher Bestandteil der Videokompression, der schon seit den frühen Jahren digitaler Videosignale verwendet wird.
Die in ihr enthaltene Entropiecodierung wurde schon mit dem ersten digitalen Videokompressionsstandard h.120, im Jahre 1984, ausgeliefert. Ein weiteres Teilstück, die Bewegungskorrektur, wurde dann 4 Jahre später, in der zweiten Version von h.120, eingeführt. Anstatt für den Menschen schlecht wahrnehmbare und somit unwichtige Bildteile zu entfernen, enfernt sie Redundanzen, sowohl in der Codierung, mittels Entropiecodierung, als auch temporaler Natur, mittels Bewegungskorrektur. Die daraus resultierende Reduzierung der Videodaten hat, in der Regel, im Gegensatz zur Irrelevanzreduktion keinen Einfluss auf die Qualität des vom Nutzer erkennbaren Bildes.
\section{Entropiecodierung}
Die Entropiecodierung ist eine verlustfreie Methode zur Kompression von Daten im Generellen. Sie findet also nicht exklusiv Anwendung in der Videokompression, bildet für sie jedoch trotzdem eine wichtige Grundlage.
\begin{align*}
h(i) = log(\frac{1}{p(i)});\quad H = \sum_{i=1}^E p(i) * h(i);\quad L = \sum_{i=1}^E p(i) * l(i)
\end{align*}
Ein Video kann als Nachricht, d.h. als Folge von Zeichen betrachtet werden. Jedes Zeichen an der Stelle i, hat dabei einen Informationsgehalt h, der sich aus seiner Eintrittswahrscheinlichkeit berechnet.

Die Entropie H ist der mittlere Informationsgehalt von einem Zeichen einer Nachricht.
Sie berechnet sich aus der Summe aller Informationsgehälter mit einer Gewichtung der Wahrscheinlichkeit des iten-Zeichens.

Da die Zeichen einer Nachricht in den meisten Fällen nicht alle mit der gleichen Wahrscheinlichkeit auftreten, ist es sinnvoll die Codierung an die Wahrscheinlichkeiten der Auftretenden Zeichen anzupassen, sodass die am häufigst auftretenden Zeichen den jeweils kürzest möglichen Code erhalten.\cite{symes_peter_digital_2004}
Die mittlere Wortlänge L berechnet sich aus der Summe aller Codelängen mit einer Gewichtung der Wahrscheinlichkeit des iten-Zeichens.

Das Shannon'sche Codierungstheorem besagt, dass, bei effizientes möglicher Kodierung mit eindeutiger Dekodierung, die mittlere Wortlänge eines Codes immer mindestens der Entropie einer Nachricht entsprechen muss. Dies bildet ein theoretisches Limit, dem sich so nah wie möglich angenähert werden soll.

Die Redundanz des Codes berechnet sich aus der Differenz von Entropie und mittlerer Wortlänge. Das Ziel für die Kompression eines Videos ist es, so wenig Code-Redundanz wie möglich zu erreichen.

Bei dem Videokompressionsstandard MPEG, wird die Entropiecodierung mittels einer Lauflängencodierung, gefolgt von einer Huffman-Codierung realisiert. Der Bitstrom eines Videos, welches zuvor quantisiert wurde, wird dabei auf aufeinanderfolgende Null-Bits untersucht. Diese werden anschließend zusammengefasst, indem ein Blockende angehängt wird.
In einer eigenen Implementierung konnte, unter Verwendung des in Abbildung \ref{fig:quantization_multi_mquants} dargestellten Originalbildes, hierdurch bereits eine Kompressionsrate von ca. 1:5 bei hoher Bildqualität erreicht werden. Bei niedrigerer Bildqualität und damit vermehrtem Auftreten von Nullen steig die Kompressionsrate auf 1:20 an (siehe Tabelle \ref{tab:test} und Anhang \ref{chap:testvorgenen}).
Im Anschluss werden im resultierenden Bitstrom, im Zuge der Huffman-Codierung, mehrfach vorkommende Zahlenfolgen basierend auf ihrer Auftrittswahrscheinlichkeit mit kürzeren Codes ersetzt.

Durch das Zusammenspiel von Irrelevanzreduktion mit Quantisierung und Redundanzreduktion mit Lauflängen-, sowie Huffman-Codierung, ist eine effektive Kompression des gesamten Bitstroms, ohne eine merkbare Verminderung der Bildqualität möglich.
% Die Kompressionsleistung sollte nach meinem Verständnis nach sehr stark vom encodeten Bild Abhängen. Ein komplett schwarzes Bild ist damit sehr gut komprimierbar, ein Bild mit vielen feinen Details dafür sehr schlecht.

\section{Bewegungskorrektur}
Alle bis jetzt vorgestellten Ansätze der Videokompression beschäftigen sich mit der Kompression von Einzelbildern innerhalb eines Videos. Bei der Bewegungskorrektur hingegen, wird jenes Kompressionspotential ausgenutzt, welches innerhalb der Abhängigkeiten der Einzelbilder in einem Video steckt.
Videos bestehen meist aus zusammenhängenden Szenen mit größtenteils unverändertem Inhalt innerhalb einer jeweils solchen Szene, den sogenannten temporalen Redundanzen.

Teilt man eine Szene in ihre Einzelbilder (Frames) auf, stellt man schnell fest, dass sich große Teile des Hintergrunds in mehreren Bildern wiederholen.
Die Bewegungskorrektur nutzt die Redundanz des Hintergrunds aus, indem es mehrfach vorhandene Teile jeweils nur ein Mal speichert und in den folgenden Bildern darauf referenziert um ein für den Zuschauer unverändertes Bild anzuzeigen.
Da Videos üblicherweise zum Großteil mit redundanten Bildteilen in einzelnen Szenen gefüllt sind, macht die von Bewegungskorrektur erzielbare Kompression einen großen Teil des gesamt möglichen Kompressionpotentials innerhalb von Videos aus.
\subsection{Frames}
Beim Codieren mittels Bewegungskorrektur werden alle Video Einzelbilder in verschiedene Bildarten aufgeteilt.
Es gibt rein intracodierte Frames, die sogenannten I-Frames. Bei ihnen handelt es sich um einzelne Vollbilder, die von keinem anderen Bild des Videos abhängen. I-Frames sind also für sich stehende Vollbilder, welche mit den üblichen Methoden der Bildkompression verkleinert wurden, somit bieten diese die geringsten Kompression.
Außerdem gibt es intercodierte Frames, die nur eine vorhergesagte Differenz des Inhaltes in Abhängigkeit zu einem vorherigen I-Frame haben, die sogenannten P-Frames.
Als letztes gibt es B-Frames, die sehr ähnlich zu P-Frames sind, jedoch in zwei Richtungen intercodiert wurden. Sie speichern nur die jeweils vorhergesagte Differenz des Inhaltes zum Vorherigen, sowie dem Nächsten I- oder P-Frame und benutzen somit den geringsten Speicherplatz.
Die Vorhersagung wird mittels einer Anpassung der Codierungsreihenfolge realisiert, sodass diese ungleich der Anzeigereihenfolge ist.\cite{symes_peter_digital_2004}

Wenn man diese Aufteilung jeweils nur einmal pro Szene anwenden würde, würden mehrere Probleme bei wahlfreiem Zugriff entstehen. So würden, wenn der I-Frame oder ein P-Frame einer Szene übersprungen wird oder gar komplett fehlt, die in den folgenden P- und B-Frames festgehaltenen Differenzen, auf den falschen I-Frame angewendet, sodass im Video  störende Artefakte entstehen, die sehr denen auf Abbildung A.3 ähneln.

Da bei einem Großteil der Anwendungsfälle von Videos jedoch ein fast vollständig wahlfreier Zugriff gewünscht ist, teilt man jede Videosequenz in mehrere kleine aufeinanderfolgende Bildergruppen (Group of pictures, kurz GOP) auf. Eine GOP wird meist mit 2 Parametern angegeben, in diesem Beispiel M und N.
Dabei ist N die Anzahl von Frames aus denen die GOP besteht, also die Distanz von einem I-Frame zum nächsten I-Frame.
M gibt die Distanz von einem I- oder P-Frame, bis zum jeweils Nächsten an, somit ist N-1 die Anzahl von B-Frames, die nach einem I- oder P-Frame folgen.\cite{huszak2010analysing}
Eine Bildergruppe fängt immer mit einem I-Frame an und wiederholt sich bis zum Ende eines Videos mit einem konstanten Schema.

Mit den Parametern M=4 und N=12, würde die GOP dann aussehen wie folgt aussehen:
\begin{align*}
\bold{I\:BBB\:P\:BBB\:P\:BBB}
\end{align*}

Bei MPEG ist eine Aufteilung mit den Parametern M=3 bis 4 und N=11 bis 15 üblich. \cite{symes_peter_digital_2004}

Betrachtet man ein Video mit einer üblichen Framerate von 25, dann ist dadurch wahlfreier Zugriff mit einer Genauigkeit von bis auf die Hälfte einer Sekunde gegeben. Außerdem wird der bei leichten Übertragungsfehlern auftretende Schaden minimiert, sodass das vom Endnutzer gesehene Video nur für sehr kurze Zeit Kompressionsartefakte aufweist, selbst bei komplettem Verlust eines I-Frames. 
\subsection{Makroblöcke}
Beim Komprimieren eines Videos wird zunächst ein Frame pro GOP mittels Irrelevanzreduktion komprimiert und dann als Referenz zwischengespeichert. Die folgenden Bilder werden, um die vom Encoder benötigte Arbeit einfach aufteilen zu können, in sogenannte Makroblöcke unterteilt. Diese Makroblöcke sind bei den meisten Standards auf eine feste Größe von 16x16 Pixel gesetzt.\cite{symes_peter_digital_2004} Die neusten Standards wie h.246/AVC unterstützen hingegen variable Blockgrößen und mehrere Refernzbilder. \cite{lin2009vlsi}
\subsection{Bewegungskorrektur}
Das in Makroblöcke aufgeteilte Bild wird Block für Block verglichen um statische Bildinhalte zu erkennen. \cite{symes_peter_digital_2004} Ein Bildinhalt ist statisch, wenn sich ein Block von einem Bild zum nächsten nicht verändert hat. Alle statischen Bildinhalte werden dann entfernt, stattdessen wird auf den Inhalt der gespeicherten Referenz verwiesen. 
Damit sind zwar statische Bildinhalte kein Problem, allerdings kann, zum Beispiel bei einem Schwenken der Kamera, der trotzdem in beiden Bildern identisch vorhandene Hintergrund nicht kodiert werden, da sich sein Block in der Referenz zum Block des folgenden Bildes unterscheidet. Um diese immer noch redundanten Bildinformationen ebenfalls entfernen zu können, bedarf es einer komplexeren Vorgehensweise, der Bewegungskorrektur.
Die Bewegungskorrektur sucht jene Blöcke im neuem Frame mittels eines Block Matching Algorithmus heraus. Gefundene Blöcke werden mit einem Vektor, der von der Position des neuen Blocks, auf die Position des Ursprungsblocks aus der Referenz zeigt, wie auf der Abbildung A.4 erkennbar, kodiert.\cite{symes_peter_digital_2004} Beim Dekodieren kann dann mittels dieser Vektoren auf die Position des alten Blocks referenziert werden, sodass nur dieser Vektor gespeichert werden muss.
