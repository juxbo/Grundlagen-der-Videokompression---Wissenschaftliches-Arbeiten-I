\chapter{Irrelevanzreduktion}
\label{kap:Irrelevanzreduktion}

% * Wie in Einleitung geschieben: Irrelevanzreduktion basiert auf Psychovisuellen Effekten. Im wesentlichen wird ausgenutzt:
% 	* Varianzen in der Helligkeit nimmt menschliches Auge besser wahr, als Varianzen im Farbton
% 	* Niedrige Ortsfrequenzen nimmt menschliches Auge besser wahr als hohe (Was zu Hölle sind Ortsfrequenzen? -> http://www.otterstedt.de/wiki/index.php/Ortsfrequenz)
% 	* Quellen: Etwas Besseres als \cite{dankmeier_grundkurs_2006} S.358 \& S.359? -> Vllt \cite{akramullah_digital_2014} S.13
% * pv. Effekte kann man sich so zu nutze machen, dass wir relevantere Informationen genauer Speichern, als weniger relevante
% * Da Daten in RAW RGB im Normalfall vorliegen müssen wir die Daten erst vorbereiten, um sie dann nutzen zu können.
% 	* Um an Helligkeit heran zu kommen -> RGB -> YUV (Oder YCrCb, wo ist der Unterschied?)
% 		* Weiterverarbeitung via Subsampling
% 	* Um an Ortsfrequenzen heran zu kommen -> DCT
% 		* Weiterverarbeitung via Quantisierung
% 		* Wobei allerdings nicht direkt weniger Speicherplatz verbraucht wird, sondern vielmehr die Daten besser komprimierbar für RLE gemacht werden, welches im nächsten Kapitel behandelt wird.

Die rohe Aufnahme eines Bildes bietet eine Fülle an Informationen. Mit Blick auf die Eigenschaften des menschlichen Sehsinns lässt sich hierbei allerdings feststellen, dass einige Informationen relevanter für das Erkennen eines Bildes sind, als andere. Die Irrelevanzreduktion beschäftigt sich mit der Trennung und Reduzierung von weniger wichtigen Informationen und bietet damit Methoden zur verlustbehafteten Datenkompression an.

Bei der Videokompression werden im wesentlichen zwei Eigenschaften zur Reduktion von Daten ausgenutzt. Zum einen nimmt das Auge Varianzen in der Helligkeit stärker wahr, als Änderungen im Farbton. Zum Anderen ist das Auge besser in der Lage niedrige Ortsfrequenzen zu erkennen, als hohe - erkennt also grobe Strukturen eher als feinere. Diese Eigenschaften können nun ausgenutzt werden, um einen guten Kompromiss aus akzeptabler Bildqualität und guter Datenreduktion zu finden \cite{akramullah_digital_2014}.
%Dies liegt vor allem an der ungleichen Verteilung von Stäben und Zapfen in der Netzhaut des menschlichen Auges.
% Oder Ortsfrequenz besser ähnlich zu http://www.otterstedt.de/wiki/index.php/Ortsfrequenz erklären?
% s/Helligkeit/Luminiszenz/g

\section{Chroma Subsampling}

Das Chroma Subsampling nutzt den Umstand aus, dass Helligkeitsvarianzen besser wahrgenommen werden, als Farbvarianzen. Zumeist liegen die Bildinformationen im Ausgangsformat jedoch im RGB Farbmodell vor, wobei hier die Helligkeitswerte in jeden Kanal eingehen. Um nun aber die Chrominanz bei gleichbleibender Auflösung der Luminanz zu reduzieren benötigen wir eine getrennte Darstellung dieser Informationen. Hierfür wird im MPEG-1 Standard die YC$_B$C$_R$ Darstellung verwendet, wobei das Y für die Luminanz steht und in C$_B$ und C$_R$ die Farbwerte codiert werden. Die Umrechnung lässt sich mittels folgender Formeln realisieren:

$
Y = 0.299 \cdot R + 0.587 \cdot G + 0.114 \cdot B \\
U = (B - Y) \cdot 0.493 \\
V = (R - Y) \cdot 0.877
$
\cite{itu-t_recommendation_1995}

Nun kann das eigentliche Subsampling stattfinden, welches bei MPEG-1 bei einer Auflösung von 4:2:0 realisiert wird. Die erste Zahl gibt hierbei die horizontale Abtastrate des Luma-Wertes an. Die zweite Zahl steht für die horizontale Abtastrate der C$_B$ und C$_R$ Kanäle in Relation zum ersten Wert. Die dritte Zahl gibt die vertikale Samplingrate an, wobei diese entweder 2 oder 0 betragen kann, also entweder kein vertikales Subsampling, oder vertikales Subsampling von 2:1 stattfindet. Für den Fall von 4:2:0 Subsampling bedeutet dies, dass jeweils 2x2 Bildpunkte des C$_B$ und C$_R$ Kanals auf einen Bildpunkt in der Ergebnismenge abgebildet werden. Hiermit wird also die Auflösung des C$_B$ und C$_R$ Kanals halbiert, was zu einer Datenreduktion von 50\% führt. \cite{poynton_chroma_????} % Wenn möglich noch Poyntons Buch "Digital video and HDTV: algorithms and interfaces" check. Gibt es über die TU bib

Das Chroma Subsampling bietet somit eine enorme Möglichkeit der Kompression, die allerdings nicht verlustfrei abläuft. Artefakte können bei Verwendung dieser Methode vor allem bei scharfen, farbigen Kanten entstehen, wenn diese durch einen gesampleten Block verlaufen. In Abbildung \ref{fig:chroma_artefacts} ist dieser Sachverhalt dargestellt. % Hell, what a sentence

\begin{figure}[h!]
    \centering
    \includegraphics[scale=10]{images/2-1_chroma_artefacts_original.png}
    \includegraphics[scale=10]{images/2-1_chroma_artefacts_sampled.png}
    \caption{Artefakte durch Chroma Subsampling}
    \textit{Links: Original, Rechts: Subsampled. Die rechte Kante des blauen Farbblocks liegt in gesubsampleten 2x2 Blöcken, wodurch Artefakte entstehen. Die linke Kante liegt zwischen zwei 2x2 Blöcken, weshalb es zu keiner falschen Darstellung kommt.}
    \label{fig:chroma_artefacts}
\end{figure}

% Referenz auf Implementierung in den Anhang?
% Beispielbild einbinden?

% Problematisch an scharfen farbigen Kanten
% * RGB -> YCbCr
% * 4:2:0 -> 50\% Komprimierung!
% * Artefakte: * Blurring, etc nach \cite{akramullah_digital_2014}



\section{Diskrete Kosinus Transformation}

* DCT ist eine spezielle Form der Fourier-Transformation
	* Fourier-Transformation aproximiert eine Funktion mittels Sinus-Funktionen
	* 4 Probleme \cite{symes_peter_digital_2004} S.71:
		* *It assumes that the time domain signal is infinite in extent*
		* *It assumes continous funtions in time*
		* Nicht ohne weiteres auf 2D anwendbar
		* Generierte Koeffitienten sind 2D (Amplitude + Phase bzw. sinus + cosine)
	* DCT funktioniert, solange nach dem Nyquist Theorem gesampled wurde (warum?)
	* Nutzt außerdem noch einen Effekt aus, an den ich mich gerade nicht mehr erinnere ->bandwidth-limited data
* DCT erlaubt uns Ortsfrequenzen zu extrahieren (warum? wodurch?)

* Formel: \[F(u,v) = \frac{1}{4} C_uC_v\sum_{x=0}^7 \sum_{y=0}^7 f(x,y) \cos \left(\frac{(2x+1)u\pi}{16}\right) \cos\left(\frac{(2y+1)v\pi}{16}\right) \]
	* Quelle \cite{symes_peter_digital_2004} S.75
* Implementierung: Siehe src/dct.py

* Es wird eine zweidimensionale DCT verwendet.

* Wann funktioniert sie nicht so gut?

\section{Quantisierung}
