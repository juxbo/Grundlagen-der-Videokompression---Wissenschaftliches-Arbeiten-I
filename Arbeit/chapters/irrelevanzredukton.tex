\chapter{Irrelevanzreduktion}
\label{kap:Irrelevanzreduktion}

% * Wie in Einleitung geschieben: Irrelevanzreduktion basiert auf Psychovisuellen Effekten. Im wesentlichen wird ausgenutzt:
% 	* Varianzen in der Helligkeit nimmt menschliches Auge besser wahr, als Varianzen im Farbton
% 	* Niedrige Ortsfrequenzen nimmt menschliches Auge besser wahr als hohe (Was zu Hölle sind Ortsfrequenzen? -> http://www.otterstedt.de/wiki/index.php/Ortsfrequenz)
% 	* Quellen: Etwas Besseres als \cite{dankmeier_grundkurs_2006} S.358 \& S.359? -> Vllt \cite{akramullah_digital_2014} S.13
% * pv. Effekte kann man sich so zu nutze machen, dass wir relevantere Informationen genauer Speichern, als weniger relevante
% * Da Daten in RAW RGB im Normalfall vorliegen müssen wir die Daten erst vorbereiten, um sie dann nutzen zu können.
% 	* Um an Helligkeit heran zu kommen -> RGB -> YUV (Oder YCrCb, wo ist der Unterschied?)
% 		* Weiterverarbeitung via Subsampling
% 	* Um an Ortsfrequenzen heran zu kommen -> DCT
% 		* Weiterverarbeitung via Quantisierung
% 		* Wobei allerdings nicht direkt weniger Speicherplatz verbraucht wird, sondern vielmehr die Daten besser komprimierbar für RLE gemacht werden, welches im nächsten Kapitel behandelt wird.

Die rohe Aufnahme eines Bildes bietet eine Fülle an Informationen. Mit Blick auf die Eigenschaften des menschlichen Sehsinns lässt sich hierbei allerdings feststellen, dass einige Informationen relevanter für das Erkennen eines Bildes sind, als andere. Die Irrelevanzreduktion beschäftigt sich mit der Trennung und Reduzierung von weniger wichtigen Informationen und bietet damit Methoden zur verlustbehafteten Datenkompression an.

Bei der Videokompression werden im wesentlichen zwei Eigenschaften zur Reduktion von Daten ausgenutzt. Zum einen nimmt das Auge Varianzen in der Helligkeit (Luminanz) stärker wahr, als Änderungen im Farbton (Chrominanz). Zum Anderen ist das Auge besser in der Lage niedrige Ortsfrequenzen zu erkennen, als hohe - erkennt also grobe Strukturen eher als feinere. Diese Eigenschaften können nun ausgenutzt werden, um einen guten Kompromiss aus akzeptabler Bildqualität und guter Datenreduktion zu finden \cite{akramullah_digital_2014}.
%Dies liegt vor allem an der ungleichen Verteilung von Stäben und Zapfen in der Netzhaut des menschlichen Auges.
% Oder Ortsfrequenz besser ähnlich zu http://www.otterstedt.de/wiki/index.php/Ortsfrequenz erklären?
% s/Helligkeit/Luminiszenz/g

\section{Chroma Subsampling}

Das Chroma Subsampling nutzt den Umstand aus, dass Helligkeitsvarianzen besser wahrgenommen werden, als Farbvarianzen. Zumeist liegen die Bildinformationen im Ausgangsformat jedoch im RGB Farbmodell vor, wobei hier die Helligkeitswerte in jeden Kanal eingehen. Um nun aber die Chrominanz bei gleichbleibender Auflösung der Luminanz zu reduzieren wird eine getrennte Darstellung dieser Informationen benötigt. Hierfür wird im MPEG-1 Standard die YC$_B$C$_R$ Darstellung verwendet, wobei das Y für die Luminanz steht und in C$_B$ und C$_R$ die Farbwerte codiert werden. Die Umrechnung lässt sich mittels folgender Formeln realisieren \cite{itu-t_recommendation_1995}:
\thickmuskip=0.5\thickmuskip
\begin{align*}
	Y = & \text{ } 0.299 \cdot R + 0.587 \cdot G + 0.114 \cdot B \\
	U = & \text{ } (B - Y) \cdot 0.493 \\
	V = & \text{ } (R - Y) \cdot 0.877
\end{align*}
Nun kann das eigentliche Subsampling stattfinden, welches bei MPEG-1 bei einer Auflösung von 4:2:0 realisiert wird. Die erste Zahl gibt hierbei die horizontale Abtastrate der Luminanz an. Die zweite Zahl steht für die horizontale Abtastrate der C$_B$ und C$_R$ Kanäle in Relation zum ersten Wert. Die dritte Zahl gibt die vertikale Sampling Rate an, wobei diese entweder 2 oder 0 betragen kann, also entweder kein vertikales Subsampling, oder vertikales Subsampling von 2:1 stattfindet. Für den Fall von 4:2:0 Subsampling bedeutet dies, dass jeweils 2x2 Bildpunkte des C$_B$ und C$_R$ Kanals auf einen Bildpunkt in der Ergebnismenge abgebildet werden. Hiermit wird also die Auflösung des C$_B$ und C$_R$ Kanals halbiert, was zu einer Datenreduktion von insgesamt 50\% führt. \cite{poynton_chroma_????} % Wenn möglich noch Poyntons Buch "Digital video and HDTV: algorithms and interfaces" check. Gibt es über die TU bib

Das Chroma Subsampling bietet somit eine gute Möglichkeit der Kompression, die allerdings nicht verlustfrei abläuft. Artefakte können, wie in Abbildung \ref{fig:chroma_artefacts} im Anhang dargestellt, bei Verwendung dieser Methode vor allem bei scharfen, farbigen Kanten entstehen, wenn diese durch einen gesubsampleten Block verlaufen.


% TODO:
% Referenz auf Implementierung in den Anhang?
% Beispielbild einbinden? check
% Einheitliche benamselung von Helligkeit/Farbwert
% Ansprache mit "wir" vermeiden
% Formel zentrieren
% Gibt es ein besseres Wort für gesubsamplet? 'unterabgetasteten' ist genauso doof

% Problematisch an scharfen farbigen Kanten
% * RGB -> YCbCr
% * 4:2:0 -> 50\% Komprimierung!
% * Artefakte: * Blurring, etc nach \cite{akramullah_digital_2014}



\section{Diskrete Kosinus Transformation}

% * DCT ist eine spezielle Form der Fourier-Transformation
% 	* Fourier-Transformation aproximiert eine Funktion mittels Sinus-Funktionen
% 	* 4 Probleme \cite{symes_peter_digital_2004} S.71:
% 		* *It assumes that the time domain signal is infinite in extent*
% 		* *It assumes continous funtions in time*
% 		* Nicht ohne weiteres auf 2D anwendbar
% 		* Generierte Koeffitienten sind 2D (Amplitude + Phase bzw. sinus + cosine)
% 	* DCT funktioniert, solange nach dem Nyquist Theorem gesampled wurde (warum?)
% 	* Nutzt außerdem noch einen Effekt aus, an den ich mich gerade nicht mehr erinnere ->bandwidth-limited data
% * DCT erlaubt uns Ortsfrequenzen zu extrahieren (warum? wodurch?)

% * Formel: \[F(u,v) = \frac{1}{4} C_uC_v\sum_{x=0}^7 \sum_{y=0}^7 f(x,y) \cos \left(\frac{(2x+1)u\pi}{16}\right) \cos\left(\frac{(2y+1)v\pi}{16}\right) \]
% 	* Quelle \cite{symes_peter_digital_2004} S.75
% * Implementierung: Siehe src/dct.py

% * Es wird eine zweidimensionale DCT verwendet.

% * Wann funktioniert sie nicht so gut?

Wie bereits oben beschrieben neigt der menschliche Sehsinn dazu niedrige Ortsfrequenzen eher zu erkennen, als höhere. Eine Ortsfrequenz ist definiert als „Anzahl bestimmter periodischer Erscheinungen bezogen auf einen räumlichen Abstand” \cite{atmwiki_ortsfrequenz_????}. Wir erkennen also gröbere Strukturen mit einer niedrigen Ortsfrequenz eher als feinere Strukturen mit einer höheren.
% Bessere Quelle finden!
Um diesen Umstand nun auszunutzen muss das Ausgangsbild von der räumlichen Ebene auf eine Frequenzebene transformiert werden, damit anschießend, in dem darauf folgenden Schritt der Quantisierung, die höheren Frequenzen reduziert werden können. Diese Transformation lässt sich mittels einer zweidimensionalen Diskreten Kosinus Transformation (DCT) bewerkstelligen.

Die DCT ist eine Sonderform der Foriertransformation, bei der eine Funktion mittels Sinusschwingungen approximiert wird. Die Fourriertransformation hat allerdings unter anderem den Nachteil, dass für jeden abgetasteten Punkt ein Tupel aus Amplitude und Phase bzw. Sinus und Kosinus Koeffizienten gespeichert werden muss. Die DCT nutzt nun den Umstand aus, dass das betrachtete Intervall begrenzt ist. Durch eine vertikale Spiegelung dieses Intervalls lassen sich die Sinus Anteile heraus kürzen, wobei am Ende lediglich Kosinus Anteile übrig bleiben, also nur ein Koeffizient pro abgetasteten Punkt gespeichert werden muss. Des Weiteren bewirkt die Spiegelung, dass Start- und Endpunkt äquivalent sind. Da die Fouriertransformation von einer unendlichen Folge ausgeht, muss der letzte Koeffizient den ggf. großen Unterschied zwischen Start- und Endpunkt ausgleichen. Sind diese Punkte aber äquivalent, wird die Kraft des letzten Koeffizienten nicht verschwendet \cite{symes_peter_digital_2004}. Eine mögliche Implementierung ist in Listing \ref{lst:impl_dct} im Anhang zu sehen. Verarbeitet werden mit der zweidimensionalen DCT immer 8x8 Blöcke eines jeden Kanals mit der Formel:

\thickmuskip=0.5\thickmuskip
\begin{align*}
  F(u,v) = \frac{1}{4} C_uC_v\sum_{x=0}^7 \sum_{y=0}^7 f(x,y) \cos \left(\frac{(2x+1)u\pi}{16}\right) \cos\left(\frac{(2y+1)v\pi}{16}\right) \\
  \text{wobei} \left\lbrace
  \begin{array}{r@{}l}
	C_u = & \text{ } \frac{1}{\sqrt{2}} \text{ für } u=0, \text{ ansonsten } C_u=1\\
	C_v = & \text{ } \frac{1}{\sqrt{2}} \text{ für } v=0, \text{ ansonsten } C_v=1
  \end{array}
  \right.
\end{align*}

Die Abbildung \ref{fig:dct-good} zeigt das Resultat einer angewandten DCT auf einen schwarz-weißen 8x8 Pixelblock, welcher aus jeweils einer horizontalen und einer vertikalen Kosinus Schwingung besteht. Der sogenannte DC Wert ist der erste Wert der Matrix und gibt die mittlere Helligkeit an. Alle anderen Komponenten beschreiben die relative Abweichung zu diesem Wert und werden gemeinhin als AC Werte betitelt, wobei diese zugleich die zum unteren rechten Rand hin höher werdenden Ortsfrequenzen repräsentieren. Wie bereits zu erkennen führt die DCT oftmals selbst schon durch Rundung auf ganzzahlige Ergebnisse zu einer Matrix mit einer erhöhten Anzahl gleicher Werte, die sich für die Anwendung weiterer, verlustfreier, Kompressionsmethoden eignet.
% Beispiel angelehnt an: https://vsr.informatik.tu-chemnitz.de/~jan/MPEG/HTML/mpeg_tech.html

\begin{figure}[h!]
    \centering
    \begin{multicols}{2}
%     \input{snippets/dct-good-luma.tex} <-- Kommt in den Anhang
    \includegraphics[scale=15]{images/2-2_dct_good.png}
    \input{snippets/dct-good-dct.tex}
    \end{multicols}
    \caption{Mittels DCT gut komprimierbarer 8x8 Pixelblock}
    \textit{Links: Ausgangsbild, Rechts: Resultierende DCT-Matrix}
    \label{fig:dct-good}
\end{figure}

% Beispiel mit niedrigen Ortsfrequenzen & Hohen Ortsfrequenzen
% DC -> Mittlere Helligkeit
% AC -> Differenz zur mittleren Helligkeit
% DCT selbst verlustfrei. Produziert in den meisten Fällen schon gut komprimierbare Daten.
% DCT = Mittels Linearkombination von Kosinus Schwingungen lässt sich ein 8x8 Pixelblock darstellen -> Vllt. in Fazit?

\section{Quantisierung}

Im vorigen Schritt wurde durch Anwendung der Diskreten Kosinus Transformation eine Matrix mit den korrespondierenden Ortsfrequenzen eines 8x8 Pixelblocks gewonnen. Um nun tatsächlich eine Reduktion der höheren Ortsfrequenzen zu erreichen wird die Methode der Quantisierung angewandt. Hierbei wird eine ganzzahlige Division eines jeden DCT Koeffizienten mit einem Quantisierungswert vorgenommen. Das gerundete Ergebnis ist dann der quantisierte Wert. Durch diese Division und Rundung wird versucht die bisher noch hohen Koeffizienten zu verkleinern, sowie in den höheren Frequenzbereichen möglichst auf Ergebnisse gleich Null zu kommen.

% Uniform quantizer -> Stufenquantisierung
Im Fall von MPEG-1 wird hierfür ein Uniform Scalar Quantizer verwendet, bei dem die Eingangswerte durch Division der Schrittgröße auf Bereiche gleicher Größe abgebildet werden, wobei eine stufenähnliche Charakteristik entsteht. \cite{symes_peter_digital_2004}. Um die errechneten Ortsfrequenzen in Relation zur Wahrnehmung des menschlichen Auges zu reduzieren wird hierfür eine Quantisierungsmatrix verwendet. Diese beinhaltet separate Werte für jeden DCT Koeffizienten. Die Schrittgröße setzt sich für AC-Werte zusammen aus dem korrespondierenden Quantisierungswert der Quantisierungsmatrix und einem Quantisierungsfaktor (MQuant). Der Quantisierungsfaktor dient der Steuerung der Bildqualität und kann einen Wert zwischen 1 und 31 annehmen, wobei ein Quantisierungsfaktor von 1 für eine hohe Bildqualität sorgt, ein Faktor von 31 hingegen für eine stark reduzierte. Da das Auge sensibel gegenüber großräumigen Luminanzfehlern ist, wird der DC durch eine feste Schrittgröße von 8 dividiert. \cite{ISO13586} Eine Implementierung des vorgestellten Algorithmus ist in Listing \ref{lst:quantizer} im Anhang zu sehen.

In Abbildung \ref{fig:quantization_multi_mquants} des Anhangs ist die angewandte Quantisierung exemplarisch an einem Beispielbild mit der im MPEG-1 Standard voreingestellten Quantisierungsmatrix (siehe Anhang, Tabelle \ref{tab:default_quant}) sowie Quantisierungsfaktoren von eins, 16 und 31 dargestellt. % Für intracoding
Bei höheren Quantisierungsfaktoren sind hier deutliche Qualitätsverluste zu erkennen, wobei die groben Strukturen des Bildes aber erhalten bleiben.

Durch die Anwendung der DCT wird ein eingehender 8x8 Pixelblock also in eine Darstellung transformiert, die es erlaubt mittels der Quantisierung vor allem enthaltene höhere Ortsfrequenzen zu reduzieren. Diese Prozesse führen zunächst jedoch nicht direkt zu einer Datenreduktion, da trotz des erhöhten Anteils gleicher Werte in der Matrix eben diese Werte auch gespeichert werden müssen. Allerdings wurde erreicht, dass die Entropiekodierung, welche im nachfolgenden Kapitel erläutert wird, bessere Kompressionsergebnisse erzielen kann.
% Default Intracoding Tabelle
% DC Wert wird mit fixed quantizer = 8 quntisiert, da luminanz voll wichtig ist
% AC Werte werden nach Formel i[u,v] =  8 * c[u,v] / (q * m[u,v]) gebildet und abgerundet
% Scaling Faktor kann während des Encoing prozesses angepasst werden
% Quantisierungsmatrix kann während des Encoing prozesses angepasst werden
